{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "2Lrl244j1N3J"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import cross_validate, KFold\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import recall_score\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = pd.read_csv('combined_data.csv')\n",
        "X = X.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "y=X['Class']\n",
        "X = X.drop('Class', axis=1)\n",
        "\n",
        "#print(X.head(4))\n",
        "#print(y.head(4))"
      ],
      "metadata": {
        "id": "2nEdpko81jA0"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SEED=42\n",
        "n_folds=5\n",
        "clf = LogisticRegression(max_iter=1000)\n",
        "\n",
        "cv = KFold(n_splits=n_folds, shuffle=True, random_state=SEED)\n",
        "\n",
        "score = cross_validate(clf, X=X, y=y, cv=cv, return_train_score=True, return_estimator=True, scoring = 'roc_auc')\n",
        "#print(\"This is the score object:\")\n",
        "#print (score)\n",
        "print(\"Average AUC test set:\", np.mean(score['test_score']))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XgQOq7_37XK_",
        "outputId": "9cb0ad7f-89bb-4bf8-c666-38b2a822512e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average AUC test set: 0.583968253968254\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import recall_score\n",
        "\n",
        "sensitivities = []\n",
        "specificities = []\n",
        "\n",
        "for i, (train_index, test_index) in enumerate(cv.split(X, y)):\n",
        "    # Get the fitted estimator for the current fold\n",
        "    fold_estimator = score['estimator'][i]\n",
        "\n",
        "    # Get the test data for the current fold\n",
        "    X_test_fold = X.iloc[test_index]\n",
        "    y_test_fold = y.iloc[test_index]\n",
        "\n",
        "    # Make predictions on the test set for the current fold\n",
        "    y_pred_fold = fold_estimator.predict(X_test_fold)\n",
        "\n",
        "    # Calculate sensitivity (recall for class 1)\n",
        "    sensitivity = recall_score(y_test_fold, y_pred_fold, pos_label=1)\n",
        "    sensitivities.append(sensitivity)\n",
        "\n",
        "    # Calculate specificity (recall for class 0)\n",
        "    specificity = recall_score(y_test_fold, y_pred_fold, pos_label=0)\n",
        "    specificities.append(specificity)\n",
        "\n",
        "print(\"Average Sensitivity after cross-validation:\", np.mean(sensitivities))\n",
        "print(\"Average Specificity after cross-validation:\", np.mean(specificities))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7QuN3chI8nMc",
        "outputId": "652a367b-0dc6-4709-ef6d-7f046cbefbb7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Sensitivity after cross-validation: 0.275\n",
            "Average Specificity after cross-validation: 0.6454761904761904\n"
          ]
        }
      ]
    }
  ]
}